Using TensorFlow backend.
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.
OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info
OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-15
OMP: Info #156: KMP_AFFINITY: 16 available OS procs
OMP: Info #157: KMP_AFFINITY: Uniform topology
OMP: Info #179: KMP_AFFINITY: 2 packages x 8 cores/pkg x 1 threads/core (16 total cores)
OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:
OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 
OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 2 
OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 3 
OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 4 
OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 8 
OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 10 
OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 11 
OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 12 
OMP: Info #171: KMP_AFFINITY: OS proc 8 maps to package 1 core 0 
OMP: Info #171: KMP_AFFINITY: OS proc 9 maps to package 1 core 2 
OMP: Info #171: KMP_AFFINITY: OS proc 10 maps to package 1 core 3 
OMP: Info #171: KMP_AFFINITY: OS proc 11 maps to package 1 core 4 
OMP: Info #171: KMP_AFFINITY: OS proc 12 maps to package 1 core 8 
OMP: Info #171: KMP_AFFINITY: OS proc 13 maps to package 1 core 10 
OMP: Info #171: KMP_AFFINITY: OS proc 14 maps to package 1 core 11 
OMP: Info #171: KMP_AFFINITY: OS proc 15 maps to package 1 core 12 
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12647 thread 0 bound to OS proc set 0
WARNING:tensorflow:From /home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

WARNING:tensorflow:From /home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

2019-11-06 15:57:00.385686: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-11-06 15:57:00.391347: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3200225000 Hz
2019-11-06 15:57:00.391597: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff5f6b6c650 executing computations on platform Host. Devices:
2019-11-06 15:57:00.391612: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-11-06 15:57:00.391694: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2019-11-06 15:57:00.550071: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12671 thread 1 bound to OS proc set 8
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12673 thread 2 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12674 thread 3 bound to OS proc set 9
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12675 thread 4 bound to OS proc set 2
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12676 thread 5 bound to OS proc set 10
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12677 thread 6 bound to OS proc set 3
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12678 thread 7 bound to OS proc set 11
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12679 thread 8 bound to OS proc set 4
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12680 thread 9 bound to OS proc set 12
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12681 thread 10 bound to OS proc set 5
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12682 thread 11 bound to OS proc set 13
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12683 thread 12 bound to OS proc set 6
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12684 thread 13 bound to OS proc set 14
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12685 thread 14 bound to OS proc set 7
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12686 thread 15 bound to OS proc set 15
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12687 thread 16 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12670 thread 17 bound to OS proc set 8
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12688 thread 18 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12689 thread 19 bound to OS proc set 9
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12690 thread 20 bound to OS proc set 2
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12691 thread 21 bound to OS proc set 10
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12692 thread 22 bound to OS proc set 3
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12693 thread 23 bound to OS proc set 11
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12694 thread 24 bound to OS proc set 4
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12695 thread 25 bound to OS proc set 12
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12696 thread 26 bound to OS proc set 5
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12698 thread 28 bound to OS proc set 6
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12697 thread 27 bound to OS proc set 13
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12699 thread 29 bound to OS proc set 14
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12700 thread 30 bound to OS proc set 7
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12701 thread 31 bound to OS proc set 15
OMP: Info #250: KMP_AFFINITY: pid 12647 tid 12702 thread 32 bound to OS proc set 0
Training on n= 30, testing on n = 030
Prior variance is 0.010000
Posterior variance for training is 0.007692
Posterior variance for testing is 0.007692
Fitting RNN with the following architecture
__________________________________________________________________________________________
Layer (type)                            Output Shape                        Param #       
==========================================================================================
simple_rnn_1 (SimpleRNN)                (None, None, 32)                    1088          
__________________________________________________________________________________________
simple_rnn_2 (SimpleRNN)                (None, None, 32)                    2080          
__________________________________________________________________________________________
simple_rnn_3 (SimpleRNN)                (None, 1)                           34            
==========================================================================================
Total params: 3,202
Trainable params: 3,202
Non-trainable params: 0
__________________________________________________________________________________________
None
Epoch 1/200

1/1 [==============================] - 1s 1s/step - loss: 67.5980 - val_loss: 11.0425
Epoch 2/200

1/1 [==============================] - 0s 70ms/step - loss: 9.5810 - val_loss: 10.4143
Epoch 3/200

1/1 [==============================] - 0s 72ms/step - loss: 9.5978 - val_loss: 7.0056
Epoch 4/200

1/1 [==============================] - 0s 72ms/step - loss: 5.5311 - val_loss: 2.3763
Epoch 5/200

1/1 [==============================] - 0s 111ms/step - loss: 2.1113 - val_loss: 0.5827
Epoch 6/200

1/1 [==============================] - 0s 71ms/step - loss: 0.5522 - val_loss: 1.3227
Epoch 7/200

1/1 [==============================] - 0s 71ms/step - loss: 1.3068 - val_loss: 2.1768
Epoch 8/200

1/1 [==============================] - 0s 72ms/step - loss: 2.1512 - val_loss: 1.9602
Epoch 9/200

1/1 [==============================] - 0s 72ms/step - loss: 1.7765 - val_loss: 1.3427
Epoch 10/200

1/1 [==============================] - 0s 72ms/step - loss: 1.3932 - val_loss: 0.8491
Epoch 11/200

1/1 [==============================] - 0s 73ms/step - loss: 0.8341 - val_loss: 0.6067
Epoch 12/200

1/1 [==============================] - 0s 89ms/step - loss: 0.6263 - val_loss: 0.5467
Epoch 13/200

1/1 [==============================] - 0s 72ms/step - loss: 0.5503 - val_loss: 0.5645
Epoch 14/200

1/1 [==============================] - 0s 73ms/step - loss: 0.5657 - val_loss: 0.5865
Epoch 15/200

1/1 [==============================] - 0s 73ms/step - loss: 0.6216 - val_loss: 0.5842
Epoch 16/200

1/1 [==============================] - 0s 71ms/step - loss: 0.7073 - val_loss: 0.5446
Epoch 17/200

1/1 [==============================] - 0s 72ms/step - loss: 0.5086 - val_loss: 0.4909
Epoch 18/200

1/1 [==============================] - 0s 72ms/step - loss: 0.5666 - val_loss: 0.4267
Epoch 19/200

1/1 [==============================] - 0s 72ms/step - loss: 0.4468 - val_loss: 0.3644
Epoch 20/200

1/1 [==============================] - 0s 72ms/step - loss: 0.3462 - val_loss: 0.3109
Epoch 21/200

1/1 [==============================] - 0s 72ms/step - loss: 0.2657 - val_loss: 0.2682
Epoch 22/200

1/1 [==============================] - 0s 72ms/step - loss: 0.2688 - val_loss: 0.2377
Epoch 23/200

1/1 [==============================] - 0s 71ms/step - loss: 0.2497 - val_loss: 0.2164
Epoch 24/200

1/1 [==============================] - 0s 72ms/step - loss: 0.2415 - val_loss: 0.1999
Epoch 25/200

1/1 [==============================] - 0s 105ms/step - loss: 0.1954 - val_loss: 0.1870
Epoch 26/200

1/1 [==============================] - 0s 73ms/step - loss: 0.1838 - val_loss: 0.1762
Epoch 27/200

1/1 [==============================] - 0s 72ms/step - loss: 0.1786 - val_loss: 0.1663
Epoch 28/200

1/1 [==============================] - 0s 72ms/step - loss: 0.1671 - val_loss: 0.1577
Epoch 29/200

1/1 [==============================] - 0s 72ms/step - loss: 0.1460 - val_loss: 0.1513
Epoch 30/200

1/1 [==============================] - 0s 72ms/step - loss: 0.1293 - val_loss: 0.1458
Epoch 31/200

1/1 [==============================] - 0s 71ms/step - loss: 0.1129 - val_loss: 0.1413
Epoch 32/200

1/1 [==============================] - 0s 71ms/step - loss: 0.1192 - val_loss: 0.1376
Epoch 33/200

1/1 [==============================] - 0s 72ms/step - loss: 0.1378 - val_loss: 0.1332
Epoch 34/200

1/1 [==============================] - 0s 75ms/step - loss: 0.1244 - val_loss: 0.1279
Epoch 35/200

1/1 [==============================] - 0s 73ms/step - loss: 0.1370 - val_loss: 0.1221
Epoch 36/200

1/1 [==============================] - 0s 72ms/step - loss: 0.1175 - val_loss: 0.1154
Epoch 37/200

1/1 [==============================] - 0s 73ms/step - loss: 0.1093 - val_loss: 0.1078
Epoch 38/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0998 - val_loss: 0.1007
Epoch 39/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0998 - val_loss: 0.0944
Epoch 40/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0907 - val_loss: 0.0890
Epoch 41/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0998 - val_loss: 0.0847
Epoch 42/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0922 - val_loss: 0.0815
Epoch 43/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0808 - val_loss: 0.0794
Epoch 44/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0742 - val_loss: 0.0782
Epoch 45/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0736 - val_loss: 0.0776
Epoch 46/200

1/1 [==============================] - 0s 105ms/step - loss: 0.0703 - val_loss: 0.0771
Epoch 47/200

1/1 [==============================] - 0s 73ms/step - loss: 0.0600 - val_loss: 0.0767
Epoch 48/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0786 - val_loss: 0.0757
Epoch 49/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0600 - val_loss: 0.0744
Epoch 50/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0718 - val_loss: 0.0731
Epoch 51/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0671 - val_loss: 0.0710
Epoch 52/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0719 - val_loss: 0.0692
Epoch 53/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0664 - val_loss: 0.0677
Epoch 54/200

1/1 [==============================] - 0s 180ms/step - loss: 0.0624 - val_loss: 0.0664
Epoch 55/200

1/1 [==============================] - 0s 75ms/step - loss: 0.0576 - val_loss: 0.0652
Epoch 56/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0518 - val_loss: 0.0642
Epoch 57/200

1/1 [==============================] - 0s 75ms/step - loss: 0.0613 - val_loss: 0.0633
Epoch 58/200

1/1 [==============================] - 0s 75ms/step - loss: 0.0712 - val_loss: 0.0621
Epoch 59/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0585 - val_loss: 0.0612
Epoch 60/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0556 - val_loss: 0.0601
Epoch 61/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0537 - val_loss: 0.0592
Epoch 62/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0502 - val_loss: 0.0582
Epoch 63/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0542 - val_loss: 0.0573
Epoch 64/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0518 - val_loss: 0.0564
Epoch 65/200

1/1 [==============================] - 0s 106ms/step - loss: 0.0555 - val_loss: 0.0556
Epoch 66/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0497 - val_loss: 0.0548
Epoch 67/200

1/1 [==============================] - 0s 83ms/step - loss: 0.0546 - val_loss: 0.0541
Epoch 68/200

1/1 [==============================] - 0s 73ms/step - loss: 0.0452 - val_loss: 0.0534
Epoch 69/200

1/1 [==============================] - 0s 73ms/step - loss: 0.0431 - val_loss: 0.0529
Epoch 70/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0491 - val_loss: 0.0524
Epoch 71/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0484 - val_loss: 0.0521
Epoch 72/200

1/1 [==============================] - 0s 73ms/step - loss: 0.0479 - val_loss: 0.0521
Epoch 73/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0427 - val_loss: 0.0517
Epoch 74/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0432 - val_loss: 0.0514
Epoch 75/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0564 - val_loss: 0.0509
Epoch 76/200

1/1 [==============================] - 0s 73ms/step - loss: 0.0398 - val_loss: 0.0504
Epoch 77/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0485 - val_loss: 0.0499
Epoch 78/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0479 - val_loss: 0.0493
Epoch 79/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0537 - val_loss: 0.0487
Epoch 80/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0459 - val_loss: 0.0480
Epoch 81/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0425 - val_loss: 0.0471
Epoch 82/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0410 - val_loss: 0.0463
Epoch 83/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0341 - val_loss: 0.0455
Epoch 84/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0400 - val_loss: 0.0450
Epoch 85/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0436 - val_loss: 0.0446
Epoch 86/200

1/1 [==============================] - 0s 107ms/step - loss: 0.0332 - val_loss: 0.0443
Epoch 87/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0394 - val_loss: 0.0440
Epoch 88/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0386 - val_loss: 0.0437
Epoch 89/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0413 - val_loss: 0.0434
Epoch 90/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0469 - val_loss: 0.0429
Epoch 91/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0381 - val_loss: 0.0423
Epoch 92/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0370 - val_loss: 0.0416
Epoch 93/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0322 - val_loss: 0.0410
Epoch 94/200

1/1 [==============================] - 0s 77ms/step - loss: 0.0348 - val_loss: 0.0403
Epoch 95/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0298 - val_loss: 0.0397
Epoch 96/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0334 - val_loss: 0.0393
Epoch 97/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0399 - val_loss: 0.0395
Epoch 98/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0328 - val_loss: 0.0393
Epoch 99/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0327 - val_loss: 0.0392
Epoch 100/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0344 - val_loss: 0.0390
Epoch 101/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0339 - val_loss: 0.0388
Epoch 102/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0333 - val_loss: 0.0385
Epoch 103/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0350 - val_loss: 0.0382
Epoch 104/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0276 - val_loss: 0.0381
Epoch 105/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0286 - val_loss: 0.0380
Epoch 106/200

1/1 [==============================] - 0s 100ms/step - loss: 0.0323 - val_loss: 0.0378
Epoch 107/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0280 - val_loss: 0.0368
Epoch 108/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0348 - val_loss: 0.0353
Epoch 109/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0332 - val_loss: 0.0338
Epoch 110/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0323 - val_loss: 0.0326
Epoch 111/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0318 - val_loss: 0.0312
Epoch 112/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0275 - val_loss: 0.0300
Epoch 113/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0289 - val_loss: 0.0297
Epoch 114/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0299 - val_loss: 0.0289
Epoch 115/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0289 - val_loss: 0.0280
Epoch 116/200

1/1 [==============================] - 0s 73ms/step - loss: 0.0280 - val_loss: 0.0273
Epoch 117/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0292 - val_loss: 0.0267
Epoch 118/200

1/1 [==============================] - 0s 76ms/step - loss: 0.0346 - val_loss: 0.0262
Epoch 119/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0267 - val_loss: 0.0259
Epoch 120/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0293 - val_loss: 0.0257
Epoch 121/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0284 - val_loss: 0.0256
Epoch 122/200

1/1 [==============================] - 0s 76ms/step - loss: 0.0253 - val_loss: 0.0254
Epoch 123/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0297 - val_loss: 0.0253
Epoch 124/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0273 - val_loss: 0.0251
Epoch 125/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0244 - val_loss: 0.0250
Epoch 126/200

1/1 [==============================] - 0s 75ms/step - loss: 0.0293 - val_loss: 0.0247
Epoch 127/200

1/1 [==============================] - 0s 113ms/step - loss: 0.0251 - val_loss: 0.0244
Epoch 128/200

1/1 [==============================] - 0s 76ms/step - loss: 0.0246 - val_loss: 0.0241
Epoch 129/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0247 - val_loss: 0.0239
Epoch 130/200

1/1 [==============================] - 0s 81ms/step - loss: 0.0239 - val_loss: 0.0239
Epoch 131/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0255 - val_loss: 0.0238
Epoch 132/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0252 - val_loss: 0.0237
Epoch 133/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0263 - val_loss: 0.0236
Epoch 134/200

1/1 [==============================] - 0s 76ms/step - loss: 0.0266 - val_loss: 0.0235
Epoch 135/200

1/1 [==============================] - 0s 75ms/step - loss: 0.0271 - val_loss: 0.0233
Epoch 136/200

1/1 [==============================] - 0s 75ms/step - loss: 0.0256 - val_loss: 0.0232
Epoch 137/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0349 - val_loss: 0.0230
Epoch 138/200

1/1 [==============================] - 0s 76ms/step - loss: 0.0213 - val_loss: 0.0228
Epoch 139/200

1/1 [==============================] - 0s 75ms/step - loss: 0.0215 - val_loss: 0.0226
Epoch 140/200

1/1 [==============================] - 0s 91ms/step - loss: 0.0251 - val_loss: 0.0224
Epoch 141/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0251 - val_loss: 0.0221
Epoch 142/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0242 - val_loss: 0.0218
Epoch 143/200

1/1 [==============================] - 0s 75ms/step - loss: 0.0225 - val_loss: 0.0217
Epoch 144/200

1/1 [==============================] - 0s 83ms/step - loss: 0.0243 - val_loss: 0.0216
Epoch 145/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0232 - val_loss: 0.0216
Epoch 146/200

1/1 [==============================] - 0s 112ms/step - loss: 0.0184 - val_loss: 0.0215
Epoch 147/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0259 - val_loss: 0.0215
Epoch 148/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0230 - val_loss: 0.0214
Epoch 149/200

1/1 [==============================] - 0s 73ms/step - loss: 0.0185 - val_loss: 0.0214
Epoch 150/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0203 - val_loss: 0.0213
Epoch 151/200

1/1 [==============================] - 0s 77ms/step - loss: 0.0211 - val_loss: 0.0213
Epoch 152/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0233 - val_loss: 0.0212
Epoch 153/200

1/1 [==============================] - 0s 81ms/step - loss: 0.0193 - val_loss: 0.0212
Epoch 154/200

1/1 [==============================] - 0s 87ms/step - loss: 0.0198 - val_loss: 0.0212
Epoch 155/200

1/1 [==============================] - 0s 83ms/step - loss: 0.0232 - val_loss: 0.0214
Epoch 156/200

1/1 [==============================] - 0s 76ms/step - loss: 0.0207 - val_loss: 0.0216
Epoch 157/200

1/1 [==============================] - 0s 75ms/step - loss: 0.0208 - val_loss: 0.0217
Epoch 158/200

1/1 [==============================] - 0s 73ms/step - loss: 0.0204 - val_loss: 0.0218
Epoch 159/200

1/1 [==============================] - 0s 75ms/step - loss: 0.0208 - val_loss: 0.0218
Epoch 160/200

1/1 [==============================] - 0s 75ms/step - loss: 0.0213 - val_loss: 0.0218
Epoch 161/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0203 - val_loss: 0.0217
Epoch 162/200

1/1 [==============================] - 0s 75ms/step - loss: 0.0220 - val_loss: 0.0218
Epoch 163/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0228 - val_loss: 0.0216
Epoch 164/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0191 - val_loss: 0.0214
Epoch 165/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0194 - val_loss: 0.0211
Epoch 166/200

1/1 [==============================] - 0s 104ms/step - loss: 0.0189 - val_loss: 0.0207
Epoch 167/200

1/1 [==============================] - 0s 75ms/step - loss: 0.0262 - val_loss: 0.0204
Epoch 168/200

1/1 [==============================] - 0s 73ms/step - loss: 0.0219 - val_loss: 0.0203
Epoch 169/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0183 - val_loss: 0.0203
Epoch 170/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0229 - val_loss: 0.0203
Epoch 171/200

1/1 [==============================] - 0s 73ms/step - loss: 0.0215 - val_loss: 0.0203
Epoch 172/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0165 - val_loss: 0.0203
Epoch 173/200

1/1 [==============================] - 0s 76ms/step - loss: 0.0178 - val_loss: 0.0203
Epoch 174/200

1/1 [==============================] - 0s 76ms/step - loss: 0.0205 - val_loss: 0.0201
Epoch 175/200

1/1 [==============================] - 0s 73ms/step - loss: 0.0165 - val_loss: 0.0199
Epoch 176/200

1/1 [==============================] - 0s 73ms/step - loss: 0.0204 - val_loss: 0.0196
Epoch 177/200

1/1 [==============================] - 0s 73ms/step - loss: 0.0202 - val_loss: 0.0192
Epoch 178/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0188 - val_loss: 0.0187
Epoch 179/200

1/1 [==============================] - 0s 73ms/step - loss: 0.0215 - val_loss: 0.0185
Epoch 180/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0211 - val_loss: 0.0183
Epoch 181/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0183 - val_loss: 0.0180
Epoch 182/200

1/1 [==============================] - 0s 79ms/step - loss: 0.0201 - val_loss: 0.0178
Epoch 183/200

1/1 [==============================] - 0s 74ms/step - loss: 0.0176 - val_loss: 0.0176
Epoch 184/200

1/1 [==============================] - 0s 75ms/step - loss: 0.0182 - val_loss: 0.0174
Epoch 185/200

1/1 [==============================] - 0s 75ms/step - loss: 0.0217 - val_loss: 0.0173
Epoch 186/200

1/1 [==============================] - 0s 111ms/step - loss: 0.0169 - val_loss: 0.0173
Epoch 187/200

1/1 [==============================] - 0s 76ms/step - loss: 0.0186 - val_loss: 0.0175
Epoch 188/200

1/1 [==============================] - 0s 73ms/step - loss: 0.0188 - val_loss: 0.0176
Epoch 189/200

1/1 [==============================] - 0s 75ms/step - loss: 0.0183 - val_loss: 0.0177
Epoch 190/200

1/1 [==============================] - 0s 73ms/step - loss: 0.0215 - val_loss: 0.0176
Epoch 191/200

1/1 [==============================] - 0s 75ms/step - loss: 0.0207 - val_loss: 0.0174
Epoch 192/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0180 - val_loss: 0.0172
Epoch 193/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0165 - val_loss: 0.0170
Epoch 194/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0197 - val_loss: 0.0168
Epoch 195/200

1/1 [==============================] - 0s 73ms/step - loss: 0.0174 - val_loss: 0.0168
Epoch 196/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0177 - val_loss: 0.0167
Epoch 197/200

1/1 [==============================] - 0s 72ms/step - loss: 0.0188 - val_loss: 0.0165
Epoch 198/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0174 - val_loss: 0.0164
Epoch 199/200

1/1 [==============================] - 0s 71ms/step - loss: 0.0183 - val_loss: 0.0163
Epoch 200/200

1/1 [==============================] - 0s 76ms/step - loss: 0.0189 - val_loss: 0.0162
