Using TensorFlow backend.
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From /home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.
OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info
OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-15
OMP: Info #156: KMP_AFFINITY: 16 available OS procs
OMP: Info #157: KMP_AFFINITY: Uniform topology
OMP: Info #179: KMP_AFFINITY: 2 packages x 8 cores/pkg x 1 threads/core (16 total cores)
OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:
OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 
OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 2 
OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 3 
OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 4 
OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 8 
OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 10 
OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 11 
OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 12 
OMP: Info #171: KMP_AFFINITY: OS proc 8 maps to package 1 core 0 
OMP: Info #171: KMP_AFFINITY: OS proc 9 maps to package 1 core 2 
OMP: Info #171: KMP_AFFINITY: OS proc 10 maps to package 1 core 3 
OMP: Info #171: KMP_AFFINITY: OS proc 11 maps to package 1 core 4 
OMP: Info #171: KMP_AFFINITY: OS proc 12 maps to package 1 core 8 
OMP: Info #171: KMP_AFFINITY: OS proc 13 maps to package 1 core 10 
OMP: Info #171: KMP_AFFINITY: OS proc 14 maps to package 1 core 11 
OMP: Info #171: KMP_AFFINITY: OS proc 15 maps to package 1 core 12 
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17565 thread 0 bound to OS proc set 0
WARNING:tensorflow:From /home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

WARNING:tensorflow:From /home/tfisher2/miniconda3/envs/keras_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

2019-11-06 15:59:54.889525: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.
2019-11-06 15:59:54.901037: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3200225000 Hz
2019-11-06 15:59:54.901115: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f6e83a2bf30 executing computations on platform Host. Devices:
2019-11-06 15:59:54.901125: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-11-06 15:59:54.901193: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2019-11-06 15:59:55.068568: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17588 thread 1 bound to OS proc set 8
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17590 thread 2 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17591 thread 3 bound to OS proc set 9
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17592 thread 4 bound to OS proc set 2
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17593 thread 5 bound to OS proc set 10
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17594 thread 6 bound to OS proc set 3
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17596 thread 8 bound to OS proc set 4
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17595 thread 7 bound to OS proc set 11
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17597 thread 9 bound to OS proc set 12
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17598 thread 10 bound to OS proc set 5
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17600 thread 12 bound to OS proc set 6
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17599 thread 11 bound to OS proc set 13
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17602 thread 14 bound to OS proc set 7
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17601 thread 13 bound to OS proc set 14
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17603 thread 15 bound to OS proc set 15
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17604 thread 16 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17587 thread 17 bound to OS proc set 8
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17605 thread 18 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17607 thread 19 bound to OS proc set 9
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17608 thread 20 bound to OS proc set 2
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17609 thread 21 bound to OS proc set 10
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17610 thread 22 bound to OS proc set 3
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17611 thread 23 bound to OS proc set 11
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17612 thread 24 bound to OS proc set 4
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17613 thread 25 bound to OS proc set 12
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17614 thread 26 bound to OS proc set 5
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17615 thread 27 bound to OS proc set 13
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17616 thread 28 bound to OS proc set 6
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17617 thread 29 bound to OS proc set 14
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17618 thread 30 bound to OS proc set 7
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17619 thread 31 bound to OS proc set 15
OMP: Info #250: KMP_AFFINITY: pid 17565 tid 17620 thread 32 bound to OS proc set 0
Training on n= 100, testing on n = 100
Prior variance is 0.010000
Posterior variance for training is 0.005000
Posterior variance for testing is 0.005000
Fitting RNN with the following architecture
__________________________________________________________________________________________
Layer (type)                            Output Shape                        Param #       
==========================================================================================
simple_rnn_1 (SimpleRNN)                (None, None, 32)                    1088          
__________________________________________________________________________________________
simple_rnn_2 (SimpleRNN)                (None, None, 32)                    2080          
__________________________________________________________________________________________
simple_rnn_3 (SimpleRNN)                (None, 1)                           34            
==========================================================================================
Total params: 3,202
Trainable params: 3,202
Non-trainable params: 0
__________________________________________________________________________________________
None
Epoch 1/200

1/1 [==============================] - 1s 1s/step - loss: 11.5681 - val_loss: 307.7159
Epoch 2/200

1/1 [==============================] - 0s 207ms/step - loss: 301.4591 - val_loss: 0.8458
Epoch 3/200

1/1 [==============================] - 0s 211ms/step - loss: 0.9191 - val_loss: 1.4699
Epoch 4/200

1/1 [==============================] - 0s 234ms/step - loss: 1.4424 - val_loss: 1.3264
Epoch 5/200

1/1 [==============================] - 0s 203ms/step - loss: 1.1515 - val_loss: 1.0532
Epoch 6/200

1/1 [==============================] - 0s 210ms/step - loss: 1.0615 - val_loss: 0.8038
Epoch 7/200

1/1 [==============================] - 0s 205ms/step - loss: 0.7942 - val_loss: 0.6293
Epoch 8/200

1/1 [==============================] - 0s 199ms/step - loss: 0.6233 - val_loss: 0.5240
Epoch 9/200

1/1 [==============================] - 0s 209ms/step - loss: 0.6431 - val_loss: 0.4548
Epoch 10/200

1/1 [==============================] - 0s 208ms/step - loss: 0.5447 - val_loss: 0.4102
Epoch 11/200

1/1 [==============================] - 0s 255ms/step - loss: 0.4689 - val_loss: 0.3849
Epoch 12/200

1/1 [==============================] - 0s 213ms/step - loss: 0.4038 - val_loss: 0.3736
Epoch 13/200

1/1 [==============================] - 0s 212ms/step - loss: 0.3730 - val_loss: 0.3688
Epoch 14/200

1/1 [==============================] - 0s 218ms/step - loss: 0.4127 - val_loss: 0.3674
Epoch 15/200

1/1 [==============================] - 0s 210ms/step - loss: 0.3821 - val_loss: 0.3691
Epoch 16/200

1/1 [==============================] - 0s 223ms/step - loss: 0.4364 - val_loss: 0.3722
Epoch 17/200

1/1 [==============================] - 0s 211ms/step - loss: 0.4279 - val_loss: 0.3747
Epoch 18/200

1/1 [==============================] - 0s 250ms/step - loss: 0.3941 - val_loss: 0.3757
Epoch 19/200

1/1 [==============================] - 0s 205ms/step - loss: 0.3295 - val_loss: 0.3758
Epoch 20/200

1/1 [==============================] - 0s 224ms/step - loss: 0.4272 - val_loss: 0.3747
Epoch 21/200

1/1 [==============================] - 0s 209ms/step - loss: 0.3769 - val_loss: 0.3707
Epoch 22/200

1/1 [==============================] - 0s 222ms/step - loss: 0.3832 - val_loss: 0.3622
Epoch 23/200

1/1 [==============================] - 0s 210ms/step - loss: 0.3566 - val_loss: 0.3512
Epoch 24/200

1/1 [==============================] - 0s 209ms/step - loss: 0.3340 - val_loss: 0.3382
Epoch 25/200

1/1 [==============================] - 0s 245ms/step - loss: 0.3571 - val_loss: 0.3242
Epoch 26/200

1/1 [==============================] - 0s 206ms/step - loss: 0.3252 - val_loss: 0.3087
Epoch 27/200

1/1 [==============================] - 0s 209ms/step - loss: 0.3239 - val_loss: 0.2933
Epoch 28/200

1/1 [==============================] - 0s 207ms/step - loss: 0.3087 - val_loss: 0.2780
Epoch 29/200

1/1 [==============================] - 0s 225ms/step - loss: 0.2576 - val_loss: 0.2627
Epoch 30/200

1/1 [==============================] - 0s 216ms/step - loss: 0.2717 - val_loss: 0.2482
Epoch 31/200

1/1 [==============================] - 0s 211ms/step - loss: 0.2249 - val_loss: 0.2350
Epoch 32/200

1/1 [==============================] - 0s 247ms/step - loss: 0.2435 - val_loss: 0.2227
Epoch 33/200

1/1 [==============================] - 0s 207ms/step - loss: 0.2200 - val_loss: 0.2109
Epoch 34/200

1/1 [==============================] - 0s 215ms/step - loss: 0.1963 - val_loss: 0.1989
Epoch 35/200

1/1 [==============================] - 0s 213ms/step - loss: 0.1874 - val_loss: 0.1874
Epoch 36/200

1/1 [==============================] - 0s 212ms/step - loss: 0.1833 - val_loss: 0.1771
Epoch 37/200

1/1 [==============================] - 0s 214ms/step - loss: 0.1787 - val_loss: 0.1685
Epoch 38/200

1/1 [==============================] - 0s 219ms/step - loss: 0.1776 - val_loss: 0.1605
Epoch 39/200

1/1 [==============================] - 0s 277ms/step - loss: 0.1556 - val_loss: 0.1535
Epoch 40/200

1/1 [==============================] - 0s 217ms/step - loss: 0.1469 - val_loss: 0.1474
Epoch 41/200

1/1 [==============================] - 0s 222ms/step - loss: 0.1605 - val_loss: 0.1420
Epoch 42/200

1/1 [==============================] - 0s 226ms/step - loss: 0.1338 - val_loss: 0.1374
Epoch 43/200

1/1 [==============================] - 0s 225ms/step - loss: 0.1637 - val_loss: 0.1330
Epoch 44/200

1/1 [==============================] - 0s 321ms/step - loss: 0.1238 - val_loss: 0.1283
Epoch 45/200

1/1 [==============================] - 0s 254ms/step - loss: 0.1206 - val_loss: 0.1236
Epoch 46/200

1/1 [==============================] - 0s 220ms/step - loss: 0.1229 - val_loss: 0.1189
Epoch 47/200

1/1 [==============================] - 0s 215ms/step - loss: 0.1133 - val_loss: 0.1144
Epoch 48/200

1/1 [==============================] - 0s 213ms/step - loss: 0.1204 - val_loss: 0.1100
Epoch 49/200

1/1 [==============================] - 0s 223ms/step - loss: 0.0929 - val_loss: 0.1062
Epoch 50/200

1/1 [==============================] - 0s 217ms/step - loss: 0.1016 - val_loss: 0.1029
Epoch 51/200

1/1 [==============================] - 0s 221ms/step - loss: 0.0918 - val_loss: 0.1001
Epoch 52/200

1/1 [==============================] - 0s 262ms/step - loss: 0.0898 - val_loss: 0.0978
Epoch 53/200

1/1 [==============================] - 0s 214ms/step - loss: 0.0936 - val_loss: 0.0958
Epoch 54/200

1/1 [==============================] - 0s 218ms/step - loss: 0.1025 - val_loss: 0.0939
Epoch 55/200

1/1 [==============================] - 0s 223ms/step - loss: 0.0955 - val_loss: 0.0921
Epoch 56/200

1/1 [==============================] - 0s 225ms/step - loss: 0.0974 - val_loss: 0.0903
Epoch 57/200

1/1 [==============================] - 0s 221ms/step - loss: 0.0836 - val_loss: 0.0885
Epoch 58/200

1/1 [==============================] - 0s 215ms/step - loss: 0.0875 - val_loss: 0.0868
Epoch 59/200

1/1 [==============================] - 0s 253ms/step - loss: 0.0882 - val_loss: 0.0851
Epoch 60/200

1/1 [==============================] - 0s 219ms/step - loss: 0.0996 - val_loss: 0.0835
Epoch 61/200

1/1 [==============================] - 0s 216ms/step - loss: 0.0699 - val_loss: 0.0821
Epoch 62/200

1/1 [==============================] - 0s 222ms/step - loss: 0.0753 - val_loss: 0.0808
Epoch 63/200

1/1 [==============================] - 0s 221ms/step - loss: 0.0835 - val_loss: 0.0796
Epoch 64/200

1/1 [==============================] - 0s 216ms/step - loss: 0.0752 - val_loss: 0.0784
Epoch 65/200

1/1 [==============================] - 0s 219ms/step - loss: 0.0760 - val_loss: 0.0772
Epoch 66/200

1/1 [==============================] - 0s 257ms/step - loss: 0.0662 - val_loss: 0.0759
Epoch 67/200

1/1 [==============================] - 0s 224ms/step - loss: 0.0659 - val_loss: 0.0745
Epoch 68/200

1/1 [==============================] - 0s 227ms/step - loss: 0.0699 - val_loss: 0.0732
Epoch 69/200

1/1 [==============================] - 0s 220ms/step - loss: 0.0768 - val_loss: 0.0717
Epoch 70/200

1/1 [==============================] - 0s 225ms/step - loss: 0.0646 - val_loss: 0.0705
Epoch 71/200

1/1 [==============================] - 0s 214ms/step - loss: 0.0750 - val_loss: 0.0691
Epoch 72/200

1/1 [==============================] - 0s 226ms/step - loss: 0.0661 - val_loss: 0.0678
Epoch 73/200

1/1 [==============================] - 0s 250ms/step - loss: 0.0617 - val_loss: 0.0667
Epoch 74/200

1/1 [==============================] - 0s 235ms/step - loss: 0.0559 - val_loss: 0.0655
Epoch 75/200

1/1 [==============================] - 0s 226ms/step - loss: 0.0561 - val_loss: 0.0644
Epoch 76/200

1/1 [==============================] - 0s 216ms/step - loss: 0.0578 - val_loss: 0.0634
Epoch 77/200

1/1 [==============================] - 0s 218ms/step - loss: 0.0577 - val_loss: 0.0623
Epoch 78/200

1/1 [==============================] - 0s 219ms/step - loss: 0.0577 - val_loss: 0.0613
Epoch 79/200

1/1 [==============================] - 0s 259ms/step - loss: 0.0513 - val_loss: 0.0603
Epoch 80/200

1/1 [==============================] - 0s 216ms/step - loss: 0.0569 - val_loss: 0.0594
Epoch 81/200

1/1 [==============================] - 0s 220ms/step - loss: 0.0557 - val_loss: 0.0585
Epoch 82/200

1/1 [==============================] - 0s 222ms/step - loss: 0.0579 - val_loss: 0.0575
Epoch 83/200

1/1 [==============================] - 0s 225ms/step - loss: 0.0458 - val_loss: 0.0565
Epoch 84/200

1/1 [==============================] - 0s 217ms/step - loss: 0.0517 - val_loss: 0.0557
Epoch 85/200

1/1 [==============================] - 0s 221ms/step - loss: 0.0541 - val_loss: 0.0548
Epoch 86/200

1/1 [==============================] - 0s 252ms/step - loss: 0.0424 - val_loss: 0.0541
Epoch 87/200

1/1 [==============================] - 0s 216ms/step - loss: 0.0479 - val_loss: 0.0534
Epoch 88/200

1/1 [==============================] - 0s 213ms/step - loss: 0.0494 - val_loss: 0.0527
Epoch 89/200

1/1 [==============================] - 0s 215ms/step - loss: 0.0508 - val_loss: 0.0520
Epoch 90/200

1/1 [==============================] - 0s 225ms/step - loss: 0.0463 - val_loss: 0.0514
Epoch 91/200

1/1 [==============================] - 0s 219ms/step - loss: 0.0462 - val_loss: 0.0507
Epoch 92/200

1/1 [==============================] - 0s 216ms/step - loss: 0.0505 - val_loss: 0.0500
Epoch 93/200

1/1 [==============================] - 0s 266ms/step - loss: 0.0422 - val_loss: 0.0493
Epoch 94/200

1/1 [==============================] - 0s 223ms/step - loss: 0.0453 - val_loss: 0.0486
Epoch 95/200

1/1 [==============================] - 0s 225ms/step - loss: 0.0377 - val_loss: 0.0479
Epoch 96/200

1/1 [==============================] - 0s 221ms/step - loss: 0.0451 - val_loss: 0.0473
Epoch 97/200

1/1 [==============================] - 0s 216ms/step - loss: 0.0510 - val_loss: 0.0466
Epoch 98/200

1/1 [==============================] - 0s 213ms/step - loss: 0.0455 - val_loss: 0.0459
Epoch 99/200

1/1 [==============================] - 0s 225ms/step - loss: 0.0427 - val_loss: 0.0452
Epoch 100/200

1/1 [==============================] - 0s 254ms/step - loss: 0.0411 - val_loss: 0.0446
Epoch 101/200

1/1 [==============================] - 0s 217ms/step - loss: 0.0402 - val_loss: 0.0439
Epoch 102/200

1/1 [==============================] - 0s 217ms/step - loss: 0.0372 - val_loss: 0.0432
Epoch 103/200

1/1 [==============================] - 0s 219ms/step - loss: 0.0400 - val_loss: 0.0424
Epoch 104/200

1/1 [==============================] - 0s 224ms/step - loss: 0.0343 - val_loss: 0.0417
Epoch 105/200

1/1 [==============================] - 0s 216ms/step - loss: 0.0394 - val_loss: 0.0410
Epoch 106/200

1/1 [==============================] - 0s 253ms/step - loss: 0.0375 - val_loss: 0.0402
Epoch 107/200

1/1 [==============================] - 0s 218ms/step - loss: 0.0402 - val_loss: 0.0395
Epoch 108/200

1/1 [==============================] - 0s 224ms/step - loss: 0.0374 - val_loss: 0.0389
Epoch 109/200

1/1 [==============================] - 0s 221ms/step - loss: 0.0359 - val_loss: 0.0383
Epoch 110/200

1/1 [==============================] - 0s 222ms/step - loss: 0.0383 - val_loss: 0.0376
Epoch 111/200

1/1 [==============================] - 0s 220ms/step - loss: 0.0342 - val_loss: 0.0371
Epoch 112/200

1/1 [==============================] - 0s 216ms/step - loss: 0.0348 - val_loss: 0.0365
Epoch 113/200

1/1 [==============================] - 0s 254ms/step - loss: 0.0294 - val_loss: 0.0360
Epoch 114/200

1/1 [==============================] - 0s 216ms/step - loss: 0.0347 - val_loss: 0.0355
Epoch 115/200

1/1 [==============================] - 0s 220ms/step - loss: 0.0334 - val_loss: 0.0351
Epoch 116/200

1/1 [==============================] - 0s 215ms/step - loss: 0.0298 - val_loss: 0.0347
Epoch 117/200

1/1 [==============================] - 0s 215ms/step - loss: 0.0363 - val_loss: 0.0343
Epoch 118/200

1/1 [==============================] - 0s 215ms/step - loss: 0.0283 - val_loss: 0.0339
Epoch 119/200

1/1 [==============================] - 0s 217ms/step - loss: 0.0304 - val_loss: 0.0335
Epoch 120/200

1/1 [==============================] - 0s 249ms/step - loss: 0.0334 - val_loss: 0.0332
Epoch 121/200

1/1 [==============================] - 0s 218ms/step - loss: 0.0309 - val_loss: 0.0328
Epoch 122/200

1/1 [==============================] - 0s 224ms/step - loss: 0.0322 - val_loss: 0.0325
Epoch 123/200

1/1 [==============================] - 0s 219ms/step - loss: 0.0287 - val_loss: 0.0321
Epoch 124/200

1/1 [==============================] - 0s 219ms/step - loss: 0.0304 - val_loss: 0.0318
Epoch 125/200

1/1 [==============================] - 0s 217ms/step - loss: 0.0324 - val_loss: 0.0314
Epoch 126/200

1/1 [==============================] - 0s 218ms/step - loss: 0.0259 - val_loss: 0.0310
Epoch 127/200

1/1 [==============================] - 0s 238ms/step - loss: 0.0259 - val_loss: 0.0307
Epoch 128/200

1/1 [==============================] - 0s 231ms/step - loss: 0.0290 - val_loss: 0.0303
Epoch 129/200

1/1 [==============================] - 0s 233ms/step - loss: 0.0308 - val_loss: 0.0300
Epoch 130/200

1/1 [==============================] - 0s 225ms/step - loss: 0.0321 - val_loss: 0.0296
Epoch 131/200

1/1 [==============================] - 0s 234ms/step - loss: 0.0268 - val_loss: 0.0292
Epoch 132/200

1/1 [==============================] - 0s 330ms/step - loss: 0.0276 - val_loss: 0.0288
Epoch 133/200

1/1 [==============================] - 0s 269ms/step - loss: 0.0294 - val_loss: 0.0284
Epoch 134/200

1/1 [==============================] - 0s 227ms/step - loss: 0.0274 - val_loss: 0.0281
Epoch 135/200

1/1 [==============================] - 0s 235ms/step - loss: 0.0261 - val_loss: 0.0277
Epoch 136/200

1/1 [==============================] - 0s 219ms/step - loss: 0.0235 - val_loss: 0.0274
Epoch 137/200

1/1 [==============================] - 0s 234ms/step - loss: 0.0262 - val_loss: 0.0270
Epoch 138/200

1/1 [==============================] - 0s 226ms/step - loss: 0.0262 - val_loss: 0.0267
Epoch 139/200

1/1 [==============================] - 0s 225ms/step - loss: 0.0260 - val_loss: 0.0264
Epoch 140/200

1/1 [==============================] - 0s 257ms/step - loss: 0.0229 - val_loss: 0.0261
Epoch 141/200

1/1 [==============================] - 0s 234ms/step - loss: 0.0251 - val_loss: 0.0258
Epoch 142/200

1/1 [==============================] - 0s 225ms/step - loss: 0.0242 - val_loss: 0.0256
Epoch 143/200

1/1 [==============================] - 0s 224ms/step - loss: 0.0219 - val_loss: 0.0253
Epoch 144/200

1/1 [==============================] - 0s 237ms/step - loss: 0.0229 - val_loss: 0.0251
Epoch 145/200

1/1 [==============================] - 0s 223ms/step - loss: 0.0250 - val_loss: 0.0248
Epoch 146/200

1/1 [==============================] - 0s 256ms/step - loss: 0.0238 - val_loss: 0.0246
Epoch 147/200

1/1 [==============================] - 0s 230ms/step - loss: 0.0222 - val_loss: 0.0243
Epoch 148/200

1/1 [==============================] - 0s 220ms/step - loss: 0.0222 - val_loss: 0.0240
Epoch 149/200

1/1 [==============================] - 0s 222ms/step - loss: 0.0212 - val_loss: 0.0237
Epoch 150/200

1/1 [==============================] - 0s 223ms/step - loss: 0.0225 - val_loss: 0.0233
Epoch 151/200

1/1 [==============================] - 0s 291ms/step - loss: 0.0197 - val_loss: 0.0229
Epoch 152/200

1/1 [==============================] - 0s 260ms/step - loss: 0.0209 - val_loss: 0.0225
Epoch 153/200

1/1 [==============================] - 0s 219ms/step - loss: 0.0199 - val_loss: 0.0221
Epoch 154/200

1/1 [==============================] - 0s 262ms/step - loss: 0.0203 - val_loss: 0.0217
Epoch 155/200

1/1 [==============================] - 0s 265ms/step - loss: 0.0205 - val_loss: 0.0214
Epoch 156/200

1/1 [==============================] - 0s 231ms/step - loss: 0.0188 - val_loss: 0.0211
Epoch 157/200

1/1 [==============================] - 0s 239ms/step - loss: 0.0194 - val_loss: 0.0208
Epoch 158/200

1/1 [==============================] - 0s 265ms/step - loss: 0.0201 - val_loss: 0.0205
Epoch 159/200

1/1 [==============================] - 0s 313ms/step - loss: 0.0207 - val_loss: 0.0202
Epoch 160/200

1/1 [==============================] - 0s 251ms/step - loss: 0.0180 - val_loss: 0.0200
Epoch 161/200

1/1 [==============================] - 0s 275ms/step - loss: 0.0188 - val_loss: 0.0197
Epoch 162/200

1/1 [==============================] - 0s 247ms/step - loss: 0.0218 - val_loss: 0.0195
Epoch 163/200

1/1 [==============================] - 0s 253ms/step - loss: 0.0220 - val_loss: 0.0192
Epoch 164/200

1/1 [==============================] - 0s 291ms/step - loss: 0.0205 - val_loss: 0.0190
Epoch 165/200

1/1 [==============================] - 0s 255ms/step - loss: 0.0201 - val_loss: 0.0187
Epoch 166/200

1/1 [==============================] - 0s 234ms/step - loss: 0.0198 - val_loss: 0.0184
Epoch 167/200

1/1 [==============================] - 0s 217ms/step - loss: 0.0164 - val_loss: 0.0182
Epoch 168/200

1/1 [==============================] - 0s 214ms/step - loss: 0.0187 - val_loss: 0.0180
Epoch 169/200

1/1 [==============================] - 0s 203ms/step - loss: 0.0190 - val_loss: 0.0178
Epoch 170/200

1/1 [==============================] - 0s 205ms/step - loss: 0.0215 - val_loss: 0.0177
Epoch 171/200

1/1 [==============================] - 0s 236ms/step - loss: 0.0152 - val_loss: 0.0175
Epoch 172/200

1/1 [==============================] - 0s 202ms/step - loss: 0.0178 - val_loss: 0.0173
Epoch 173/200

1/1 [==============================] - 0s 204ms/step - loss: 0.0184 - val_loss: 0.0171
Epoch 174/200

1/1 [==============================] - 0s 208ms/step - loss: 0.0181 - val_loss: 0.0169
Epoch 175/200

1/1 [==============================] - 0s 206ms/step - loss: 0.0174 - val_loss: 0.0167
Epoch 176/200

1/1 [==============================] - 0s 208ms/step - loss: 0.0188 - val_loss: 0.0165
Epoch 177/200

1/1 [==============================] - 0s 209ms/step - loss: 0.0161 - val_loss: 0.0163
Epoch 178/200

1/1 [==============================] - 0s 233ms/step - loss: 0.0171 - val_loss: 0.0162
Epoch 179/200

1/1 [==============================] - 0s 205ms/step - loss: 0.0183 - val_loss: 0.0161
Epoch 180/200

1/1 [==============================] - 0s 203ms/step - loss: 0.0175 - val_loss: 0.0159
Epoch 181/200

1/1 [==============================] - 0s 204ms/step - loss: 0.0161 - val_loss: 0.0158
Epoch 182/200

1/1 [==============================] - 0s 205ms/step - loss: 0.0187 - val_loss: 0.0157
Epoch 183/200

1/1 [==============================] - 0s 207ms/step - loss: 0.0171 - val_loss: 0.0156
Epoch 184/200

1/1 [==============================] - 0s 205ms/step - loss: 0.0148 - val_loss: 0.0155
Epoch 185/200

1/1 [==============================] - 0s 210ms/step - loss: 0.0158 - val_loss: 0.0154
Epoch 186/200

1/1 [==============================] - 0s 232ms/step - loss: 0.0158 - val_loss: 0.0153
Epoch 187/200

1/1 [==============================] - 0s 210ms/step - loss: 0.0170 - val_loss: 0.0152
Epoch 188/200

1/1 [==============================] - 0s 204ms/step - loss: 0.0157 - val_loss: 0.0152
Epoch 189/200

1/1 [==============================] - 0s 203ms/step - loss: 0.0150 - val_loss: 0.0151
Epoch 190/200

1/1 [==============================] - 0s 207ms/step - loss: 0.0164 - val_loss: 0.0150
Epoch 191/200

1/1 [==============================] - 0s 204ms/step - loss: 0.0152 - val_loss: 0.0149
Epoch 192/200

1/1 [==============================] - 0s 209ms/step - loss: 0.0181 - val_loss: 0.0149
Epoch 193/200

1/1 [==============================] - 0s 239ms/step - loss: 0.0145 - val_loss: 0.0148
Epoch 194/200

1/1 [==============================] - 0s 207ms/step - loss: 0.0152 - val_loss: 0.0148
Epoch 195/200

1/1 [==============================] - 0s 216ms/step - loss: 0.0150 - val_loss: 0.0147
Epoch 196/200

1/1 [==============================] - 0s 214ms/step - loss: 0.0170 - val_loss: 0.0147
Epoch 197/200

1/1 [==============================] - 0s 208ms/step - loss: 0.0159 - val_loss: 0.0147
Epoch 198/200

1/1 [==============================] - 0s 207ms/step - loss: 0.0132 - val_loss: 0.0146
Epoch 199/200

1/1 [==============================] - 0s 205ms/step - loss: 0.0149 - val_loss: 0.0146
Epoch 200/200

1/1 [==============================] - 0s 232ms/step - loss: 0.0139 - val_loss: 0.0146
